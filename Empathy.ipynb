{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>class</th>\n",
       "      <th>response_text</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>response_1</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>response_2</td>\n",
       "      <td>flagged</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>response_3</td>\n",
       "      <td>flagged</td>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>response_4</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>response_5</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>response_6</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>a couple of years ago my friends was going to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>response_7</td>\n",
       "      <td>flagged</td>\n",
       "      <td>Roommate when he was going through death and l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>response_8</td>\n",
       "      <td>flagged</td>\n",
       "      <td>i've had a couple of friends (you could say mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>response_9</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Listened to someone talk about relationship tr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>response_10</td>\n",
       "      <td>flagged</td>\n",
       "      <td>I will always listen. I comforted my sister wh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>response_11</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Took a week off work, packed up the car and pi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>response_12</td>\n",
       "      <td>flagged</td>\n",
       "      <td>On the memorial anniversary of my friends fath...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>response_13</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Anxious girlfriend always needs my help</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>response_14</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>Never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>response_15</td>\n",
       "      <td>not_flagged</td>\n",
       "      <td>You as a mom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    response_id        class  \\\n",
       "0    response_1  not_flagged   \n",
       "1    response_2      flagged   \n",
       "2    response_3      flagged   \n",
       "3    response_4  not_flagged   \n",
       "4    response_5  not_flagged   \n",
       "5    response_6  not_flagged   \n",
       "6    response_7      flagged   \n",
       "7    response_8      flagged   \n",
       "8    response_9  not_flagged   \n",
       "9   response_10      flagged   \n",
       "10  response_11  not_flagged   \n",
       "11  response_12      flagged   \n",
       "12  response_13  not_flagged   \n",
       "13  response_14  not_flagged   \n",
       "14  response_15  not_flagged   \n",
       "\n",
       "                                        response_text Unnamed: 3  Unnamed: 4  \\\n",
       "0               I try and avoid this sort of conflict        NaN         NaN   \n",
       "1   Had a friend open up to me about his mental ad...        NaN         NaN   \n",
       "2   I saved a girl from suicide once. She was goin...        NaN         NaN   \n",
       "3   i cant think of one really...i think i may hav...        NaN         NaN   \n",
       "4   Only really one friend who doesn't fit into th...                    NaN   \n",
       "5   a couple of years ago my friends was going to ...        NaN         NaN   \n",
       "6   Roommate when he was going through death and l...        NaN         NaN   \n",
       "7   i've had a couple of friends (you could say mo...        NaN         NaN   \n",
       "8   Listened to someone talk about relationship tr...        NaN         NaN   \n",
       "9   I will always listen. I comforted my sister wh...        NaN         NaN   \n",
       "10  Took a week off work, packed up the car and pi...        NaN         NaN   \n",
       "11  On the memorial anniversary of my friends fath...        NaN         NaN   \n",
       "12            Anxious girlfriend always needs my help        NaN         NaN   \n",
       "13                                              Never        NaN         NaN   \n",
       "14                                       You as a mom        NaN         NaN   \n",
       "\n",
       "   Unnamed: 5  Unnamed: 6 Unnamed: 7  \n",
       "0         NaN         NaN        NaN  \n",
       "1         NaN         NaN        NaN  \n",
       "2         NaN         NaN        NaN  \n",
       "3         NaN         NaN        NaN  \n",
       "4         NaN         NaN        NaN  \n",
       "5         NaN         NaN        NaN  \n",
       "6         NaN         NaN        NaN  \n",
       "7         NaN         NaN        NaN  \n",
       "8         NaN         NaN        NaN  \n",
       "9         NaN         NaN        NaN  \n",
       "10        NaN         NaN        NaN  \n",
       "11        NaN         NaN        NaN  \n",
       "12        NaN         NaN        NaN  \n",
       "13        NaN         NaN        NaN  \n",
       "14        NaN         NaN        NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('Sheet_1.csv')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "desc_lines = []\n",
    "lines = df['response_text'].values.tolist()\n",
    "\n",
    "for line in lines :\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    desc_lines.append(words)\n",
    "    len(desc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1116 12:10:59.312075 13356 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 551\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences = desc_lines, size = 512, window = 5, workers = 6, min_count = 1)\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size %d' %len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sheet1_embedding.txt'\n",
    "model.wv.save_word2vec_format(filename, binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'sheet1_embedding.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['response_text']\n",
    "Y = df['class']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 22)\n",
    "\n",
    "max_length = max([len(s.split()) for s in X])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tkn = Tokenizer()\n",
    "tkn.fit_on_texts(desc_lines)\n",
    "sequences = tkn.texts_to_sequences(desc_lines)\n",
    "\n",
    "word_index = tkn.word_index\n",
    "print(\"Found %s unique tokens\" %len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 551 unique tokens\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def one_hot():\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df['class'])\n",
    "    #list(le.classes_))\n",
    "    var = list(le.transform(df['class']))\n",
    "    var = to_categorical(var)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of description tensor : (80, 304)\n",
      "Shape of class tensor : (80, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of description tensor :\", review_pad.shape)\n",
    "print(\"Shape of class tensor :\", one_hot().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n"
     ]
    }
   ],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 512))\n",
    "\n",
    "for word, i in word_index.items() :\n",
    "    if i > num_words :\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None :\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 304, 512)          282624    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 304, 128)          328192    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 304, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 90)                78840     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 90)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 182       \n",
      "=================================================================\n",
      "Total params: 689,838\n",
      "Trainable params: 407,214\n",
      "Non-trainable params: 282,624\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
    "from keras.layers.embeddings import Embedding \n",
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words, 512, \n",
    "                            embeddings_initializer = Constant(embedding_matrix), \n",
    "                            input_length = max_length,\n",
    "                            trainable = False\n",
    "                           )\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(90, return_sequences = False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer ='adam', loss='binary_crossentropy', metrics =['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 304)\n",
      "(24, 2)\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.3\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "num_validation_samples = int(validation_split * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "\n",
    "print(X_test_pad.shape)\n",
    "\n",
    "Y_train_oh = one_hot()[:-num_validation_samples]\n",
    "Y_test_oh = one_hot()[-num_validation_samples:]\n",
    "\n",
    "print(Y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "56/56 [==============================] - 7s 122ms/step - loss: 0.6884 - acc: 0.6339\n",
      "Epoch 2/25\n",
      "56/56 [==============================] - 5s 92ms/step - loss: 0.6586 - acc: 0.6964\n",
      "Epoch 3/25\n",
      "56/56 [==============================] - 5s 97ms/step - loss: 0.6549 - acc: 0.6964\n",
      "Epoch 4/25\n",
      "56/56 [==============================] - 6s 100ms/step - loss: 0.6318 - acc: 0.6964\n",
      "Epoch 5/25\n",
      "56/56 [==============================] - 5s 97ms/step - loss: 0.6090 - acc: 0.6964\n",
      "Epoch 6/25\n",
      "56/56 [==============================] - 5s 97ms/step - loss: 0.6347 - acc: 0.6964\n",
      "Epoch 7/25\n",
      "56/56 [==============================] - 6s 100ms/step - loss: 0.6222 - acc: 0.6964\n",
      "Epoch 8/25\n",
      "56/56 [==============================] - 6s 105ms/step - loss: 0.6277 - acc: 0.6964\n",
      "Epoch 9/25\n",
      "56/56 [==============================] - 6s 111ms/step - loss: 0.6177 - acc: 0.6964\n",
      "Epoch 10/25\n",
      "56/56 [==============================] - 6s 102ms/step - loss: 0.6170 - acc: 0.6964\n",
      "Epoch 11/25\n",
      "56/56 [==============================] - 6s 100ms/step - loss: 0.6216 - acc: 0.6964\n",
      "Epoch 12/25\n",
      "56/56 [==============================] - 5s 98ms/step - loss: 0.6228 - acc: 0.6964\n",
      "Epoch 13/25\n",
      "56/56 [==============================] - 6s 103ms/step - loss: 0.6252 - acc: 0.6964\n",
      "Epoch 14/25\n",
      "56/56 [==============================] - 6s 107ms/step - loss: 0.6077 - acc: 0.6964\n",
      "Epoch 15/25\n",
      "56/56 [==============================] - 6s 107ms/step - loss: 0.6038 - acc: 0.6964\n",
      "Epoch 16/25\n",
      "56/56 [==============================] - 7s 131ms/step - loss: 0.6154 - acc: 0.6964\n",
      "Epoch 17/25\n",
      "56/56 [==============================] - 7s 123ms/step - loss: 0.6134 - acc: 0.6964\n",
      "Epoch 18/25\n",
      "56/56 [==============================] - 9s 153ms/step - loss: 0.6215 - acc: 0.6964\n",
      "Epoch 19/25\n",
      "56/56 [==============================] - 6s 108ms/step - loss: 0.6203 - acc: 0.6964\n",
      "Epoch 20/25\n",
      "56/56 [==============================] - 7s 128ms/step - loss: 0.6271 - acc: 0.6964\n",
      "Epoch 21/25\n",
      "56/56 [==============================] - 6s 102ms/step - loss: 0.6082 - acc: 0.6964\n",
      "Epoch 22/25\n",
      "56/56 [==============================] - 6s 112ms/step - loss: 0.6192 - acc: 0.6964\n",
      "Epoch 23/25\n",
      "56/56 [==============================] - 6s 104ms/step - loss: 0.6036 - acc: 0.6964\n",
      "Epoch 24/25\n",
      "56/56 [==============================] - 7s 124ms/step - loss: 0.6210 - acc: 0.6964\n",
      "Epoch 25/25\n",
      "56/56 [==============================] - 7s 123ms/step - loss: 0.6176 - acc: 0.6964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a6892fe8c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, Y_train_oh, epochs = 25, batch_size = 8, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
